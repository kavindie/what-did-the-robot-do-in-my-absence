## [Kavindie Katuwandeniya](https://people.csiro.au/k/k/kavi-katuwandeniya), [Leimin Tian](https://tianleimin.github.io/), and [Dana Kulic](https://www.monash.edu/engineering/danakulic).

[Preprint can be found in arxiv.](https://arxiv.org/abs/2411.10016)

## Abstract
This paper investigates the application of Video
Foundation Models (ViFMs) for generating robot data summaries
to enhance intermittent human supervision of robot
teams. We propose a novel framework that produces both
generic and query-driven summaries of long-duration robot
vision data in three modalities: storyboards, short videos,
and text. Through a user study involving 30 participants, we
evaluate the efficacy of these summary methods in allowing
operators to accurately retrieve the observations and actions
that occurred while the robot was operating without supervision
over an extended duration (40 min). Our findings reveal that
query-driven summaries significantly improve retrieval accuracy
compared to generic summaries or raw data, albeit with
increased task duration. Storyboards are found to be the most
effective presentation modality, especially for object-related
queries. This work represents, to our knowledge, the first zeroshot
application of ViFMs for generating multi-modal robotto-
human communication in intermittent supervision contexts,
demonstrating both the promise and limitations of these models
in human-robot interaction (HRI) scenarios.

![image](https://github.com/user-attachments/assets/574afb0f-dd58-4f84-ac13-f34dba0f0bff)
Fig. 1: Diagram of the proposed robot summary generation system. The system generates generic or query-driven summaries of long egocentric robot videos in the form of
storyboards, short videos, or text to help a user review the robotsâ€™ autonomous history.
